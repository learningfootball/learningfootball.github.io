[{"fontsizes": ["6.97"], "fontnames": ["ZBMZDU+LinBiolinumT"], "x1": 35.582, "y1": 764.3992094, "x2": 191.5091942, "y2": 771.3730094, "text": "Article, August, 2022, Synthetic Football Training Data"}, {"fontsizes": ["6.97"], "fontnames": ["ZBMZDU+LinBiolinumT"], "x1": 429.075, "y1": 764.3992094, "x2": 576.2012586, "y2": 771.3730094, "text": "Jose Cerqueira Fernandes and Benjamin Kenwright"}, {"fontsizes": ["9.17", "8.97"], "fontnames": ["XNLJOV+LinLibertineT", "AKXPHQ+LinLibertineTB"], "x1": 35.798, "y1": 688.143, "x2": 301.0198142336, "y2": 748.9166, "text": "[Herold et al. 2019]. Most of these data extraction projects have focused\non understanding the game mechanics (predicting and understanding\nthe game) [Herold et al. 2019; Rommers et al. 2020].\nOur work: As far as we are aware no other work has attempted to\nfully generate a synthetic training set using computer graphics for use\non real-world football game feature extraction."}, {"fontsizes": ["8.97"], "fontnames": ["AKXPHQ+LinLibertineTB"], "x1": 35.502, "y1": 436.3587616, "x2": 302.5506953984, "y2": 489.1601616, "text": "Figure 2: Network Architecture - Keypoint extraction using a CNN.\nThe figure summarizes the blocks of the convolution layers of the\napproach presented in this paper. For example, given a input of\n256x256 pixels we extracted 52 dimensional features (26 x-y points\nidentifying key locations on the pitch - as shown in Figure 4)."}, {"fontsizes": ["10.91", "8.97", "9.17"], "fontnames": ["AKXPHQ+LinLibertineTB", "XNLJOV+LinLibertineT", "URWIFX+txsys"], "x1": 35.459, "y1": 229.04082879999999, "x2": 302.564491168, "y2": 412.56809790000005, "text": "3 Method\nTraining methodology We rendered multiple image datasets with a fo-\ncus on specific features. During training, we perform data augmentation\nwith overlay images. These augmentations were especially important for\nsynthetic images which would otherwise be free of imperfections (noise\nand other artifacts). While some of these could be done at render time,\nwe performed them at training time in order to randomly apply different\naugmentations to the same training images. We implemented neural net-\nworks with PyTorch and trained them with the Adam optimizer. Since\nthe football pitch sits on a flat ground plan, a top down layout was used\nto define the pitch keypoint markers (shown in Figure 4).\nTraining Data and Test Cases The experiments comprised of different\ndatasets with 3000 images for training and 100 images for testing. The\ndifferent datasets evaluated the value and impact of different synthetic\nfeatures (for example, the value of adding or removing lighting from a\nsynthetic scene also the importance of a realistic surroundings vs blank\nor randomly placed graphics).\n\u2022 Flat pitch (blank background, no lighting or players) as shown in"}, {"fontsizes": ["9.17"], "fontnames": ["XNLJOV+LinLibertineT"], "x1": 44.339, "y1": 221.262, "x2": 76.7210648, "y2": 230.4276, "text": "Figure 4."}, {"fontsizes": ["9.17"], "fontnames": ["URWIFX+txsys", "XNLJOV+LinLibertineT"], "x1": 35.798, "y1": 210.24282879999998, "x2": 301.01498294000004, "y2": 220.8016, "text": "\u2022 Pitch with random pictures in the background (add noise and feature"}, {"fontsizes": ["9.17"], "fontnames": ["XNLJOV+LinLibertineT"], "x1": 44.339, "y1": 202.01, "x2": 147.02121680000002, "y2": 211.1756, "text": "distractions to the training)"}, {"fontsizes": ["9.17"], "fontnames": ["URWIFX+txsys", "XNLJOV+LinLibertineT"], "x1": 35.798, "y1": 144.935, "x2": 302.0248442447999, "y2": 201.7196, "text": "\u2022 Pitch with random lighting conditions\n\u2022 Pitch players randomly placed on pitch\n\u2022 Pitch with stadium model and random lighting conditions\n\u2022 Samples with visual artifacts added to simulate various conditions\n(e.g., weather or image corruption), using overlay textures (to noise,\ncorruption, blurring)."}, {"fontsizes": ["8.97", "9.17"], "fontnames": ["AKXPHQ+LinLibertineTB", "XNLJOV+LinLibertineT"], "x1": 35.358, "y1": 35.015, "x2": 302.56920392, "y2": 140.65560000000002, "text": "Landmark (Pitch) localization Landmark localization finds the po-\nsition for the pitch (2d locations on screen and their corresponding\n3-dimensional locations relative to the camera). We evaluate our ap-\nproach using synthetic data (both controlled test cases and generate ones\nto evaluate a diverse range of problems around extracting features from\nfootball images).\nThe trained network used a mean squared error loss to directly predict.\nWe use the provided feature points to extract and identify regions of\ninterest from the image (e.g., pixel regions-of-interest from each image).\nAs shown in Figure 4, the ground markers provide essential details\n(e.g., location of players during the game). Importantly, multiple image"}, {"fontsizes": ["9.17", "8.97"], "fontnames": ["XNLJOV+LinLibertineT", "AKXPHQ+LinLibertineTB"], "x1": 310.642, "y1": 452.836, "x2": 577.613432, "y2": 748.9166, "text": "sources could be used to extrapolate the complete game data, which\nmeans, different images sources may only show specific regions of the\ngame at specific times (including different viewing angles, lighting, image\nartifacts and so on). Most of the time from live media footage of football\ngames, it is rare to see all the players and all the information in high\nresolution at the same time in a single image.\n2-dimensional data to 3-dimensional data The football pitch is flat\nand conforms to a international standard, so the 2d pitch key-point\ndata can be used as a point of reference for reconstructing a relative\n3-dimensional layout of the game. This data feeds through to player\nposition data, using the key-point data lets us calculate the location of\nthe player in real-world coordinates.\nOur synthetic football images are realistic, diverse and scalable. Starting\nwith a basic pitch template, we randomize features (orientation and\nviewing position), random placement of objects and backgrounds, and\nthe \u2018visual\u2019 characteristics (lighting and weather conditions). We finally\nrender the scene to a database with the associated keypoint data (i.e., the\nlocation features within the image).\nPlayer Body/Pose Detection There is already a substantial body of\nwork that has focused specifically on the detection and extraction of\nhuman poses (describing the bounding area and body keypoints). For the\ntests in this paper, we used the PyTorch ResNet101 pre-trained model\nwhich produced acceptable results (and could be swapped out for other\nversions at a later date). For example, the DeepCut model provides a\nrobust solution for multiple close proximity interactions [Pishchulin et al.\n2016].\nTo ensure the body/pose phase did not detect individuals in the crowd\n(only providing details for the players), after the football pitch outer\nmarker regions were detected, these were used to \u2018cull\u2019 the image to the\nfield area (see Figure 1."}, {"fontsizes": ["10.91", "9.17"], "fontnames": ["AKXPHQ+LinLibertineTB", "XNLJOV+LinLibertineT", "URWIFX+txsys"], "x1": 310.642, "y1": 277.6648288, "x2": 577.75220392, "y2": 439.15809790000003, "text": "4 Results\nOnly synthetic (generated) data was used for training. The generated im-\nage datasets where created using web-based tools (i.e., WebGPU API for\nhardware acceleration [Kenwright 2022]). We used a web-based solution\ndue to the fact that new technologies offered by modern browsers have\ngreatly increased in capabilities, not to mention, ubiquity across plat-\nforms, easy to make iterative updates (during early exploratory stages).\nThe WebGPU API [Kenwright 2022] offered GPU-accelerated client-side\nrendering within the browser (generated 3000 images within a few min-\nutes). The training of the network was done offline using PyTorch. While\nwe experimented with different network configurations initially, the final\nnetwork topology is shown in Figure 2. This configuration was used\nfor all of the tests and final results. The web-based resources, generated\ntraining data and scripts are accessible online (Git Repository [Project\nRepository 2022]).\n\u2022 Synthetic images for training (sample set for training and a corre-"}, {"fontsizes": ["9.17"], "fontnames": ["XNLJOV+LinLibertineT"], "x1": 319.522, "y1": 269.432, "x2": 483.98952639999993, "y2": 278.5976, "text": "sponding set for validating/checking errors)"}, {"fontsizes": ["9.17"], "fontnames": ["URWIFX+txsys", "XNLJOV+LinLibertineT"], "x1": 310.981, "y1": 258.05582880000003, "x2": 577.754347648, "y2": 268.6146, "text": "\u2022 Simple \u2018clean\u2019 image dataset (perfect images with no noise or incon-"}, {"fontsizes": ["9.17"], "fontnames": ["XNLJOV+LinLibertineT"], "x1": 319.522, "y1": 249.823, "x2": 526.3620952, "y2": 258.9886, "text": "sistencies) - first check to ensure the system is working"}, {"fontsizes": ["9.17"], "fontnames": ["URWIFX+txsys", "XNLJOV+LinLibertineT"], "x1": 310.981, "y1": 220.589, "x2": 577.7539443616, "y2": 249.00560000000002, "text": "\u2022 Complex and diverse dataset (larger range of views) - including \u2018am-\nbiguous\u2019 scenes (difficult to extract and identify the pitch data from\nwhat the image shows)"}, {"fontsizes": ["9.17"], "fontnames": ["URWIFX+txsys", "XNLJOV+LinLibertineT"], "x1": 310.981, "y1": 172.442, "x2": 577.7468960151999, "y2": 220.29860000000002, "text": "\u2022 Mix lighting conditions and noise\n\u2022 Add objects and players (ball and players on pitch)\n\u2022 Overlay \u2018identified\u2019 features on top of \u2019real-world\u2019 data images (com-\npare visually the errors). Such as the location of field markers and the\ncamera frustum."}, {"fontsizes": ["9.17"], "fontnames": ["URWIFX+txsys", "XNLJOV+LinLibertineT"], "x1": 310.981, "y1": 161.8838288, "x2": 576.205727872, "y2": 172.4426, "text": "\u2022 Use different colors to visualize the information (ball, players and the"}, {"fontsizes": ["9.17"], "fontnames": ["XNLJOV+LinLibertineT"], "x1": 319.522, "y1": 153.651, "x2": 341.244472, "y2": 162.81660000000002, "text": "pitch)"}, {"fontsizes": ["9.17", "8.97"], "fontnames": ["XNLJOV+LinLibertineT", "AKXPHQ+LinLibertineTB"], "x1": 310.642, "y1": 33.035, "x2": 577.7521764232, "y2": 150.5456, "text": "Evaluating the landmark detectors from the image database, which ex-\nhibits environmental, lighting and camera information, we see that mod-\nels trained with synthetic data alone can generalize well to real data\nof diverse sources (broadcast media). In order to generate diverse syn-\nthetic data, our generative models must be trained with diverse range of\nconfigurations.\nNetwork Architecture The final network architecture configuration\nwas reduced to a reasonable size using a combinations of Conv2d and\nMaxPool2d filter layers before using a series of Dense layers. It required\n5 Conv2d+MaxPool2d layers to bring the image dimension down to an\nacceptable size. Dropoff was added between each layer in order to reduce\noverfitting (increasing the dropoff probability gradually in later layers)."}]