{"version":"0.0.2","signiture":"webgpulab","info":"https://webgpu.xbdev.net","date":"2022-06-29T08:26:24.693Z","author":"Kenwright","uniqueid":"wET19ECncjU5S0eu7fqiccaEHwphGgc5","tasks":[{"tabname":"index.js","tabcontents":"/*\n  Lighting \n*/\nconsole.log('index.js');\n\ndocument.body.style.margin  = '0';\ndocument.body.style.padding = '0';\n\n/*\nvar script = document.createElement('script');\nscript.type  = 'text/javascript';\nscript.src   = 'https://cdnjs.cloudflare.com/ajax/libs/gl-matrix/2.6.0/gl-matrix-min.js';\ndocument.head.appendChild(script); \n*/\n\nlet pp = await fetch( 'https://cdnjs.cloudflare.com/ajax/libs/gl-matrix/2.6.0/gl-matrix-min.js' );\nlet tt = await pp.text();\n\nvar script = document.createElement('script');\nscript.type  = 'text/javascript';\n//script.src   = 'https://cdnjs.cloudflare.com/ajax/libs/gl-matrix/2.6.0/gl-matrix-min.js';\nscript.innerHTML = tt;\ndocument.head.appendChild(script); \n\n\nconst canvas = document.createElement('canvas');\ndocument.body.appendChild( canvas );\ncanvas.width  = canvas.height = 256;\nconsole.log( canvas.width, canvas.height );\nconst context = canvas.getContext('webgpu');\ncanvas.style.position = 'absolute';\ncanvas.style.left = '0px';\ncanvas.style.top  = '0px';\n\n\n\n\nconst gpu = navigator.gpu;\nconsole.log( 'gpu:', gpu );\n\nconst adapter = await gpu.requestAdapter();\nconst device  = await adapter.requestDevice();\n\nconst presentationSize = [ canvas.clientWidth, canvas.clientHeight  ];\nconst presentationFormat = context.getPreferredFormat(adapter);\nconsole.log( presentationFormat  );\n\ncontext.configure({ device: device,\n                    format: presentationFormat,\n                    size  : presentationSize });\n\n\nvar positions =  new Float32Array([\n   -1,0,1,  -1,0,-1,   1,0,-1,   1,0,1 ]);\n\nvar indices = new Uint32Array([\n    0,1,2,    2,3,0 ]);\n  \nvar colors  = new Float32Array([\n    0,1,0, 0,1,0, 0,1,0,  0,1,0 ]);\n\nvar normals = new Float32Array([\n    0,1,0,  0,1,0,  0,1,0,  0,1,0]);\n\nvar uvs  = new Float32Array([\n    0,0, 1,0, 1,1, 0,1 ]);\n\n\n\n\n\n\nconst positionBuffer = device.createBuffer({\n  size:  positions.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST\n});\n\nconst colorBuffer = device.createBuffer({\n  size:  colors.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST\n});\n\nconst normalBuffer = device.createBuffer({\n  size:  normals.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST\n});\n\nconst uvBuffer = device.createBuffer({\n  size:  uvs.byteLength,\n  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST\n});\n\nconst indicesBuffer = device.createBuffer({\n  size:  indices.byteLength,\n  usage: GPUBufferUsage.INDEX | GPUBufferUsage.COPY_DST\n});\n\n\ndevice.queue.writeBuffer(positionBuffer, 0, positions);\ndevice.queue.writeBuffer(colorBuffer   , 0, colors   );\ndevice.queue.writeBuffer(normalBuffer  , 0, normals  );\ndevice.queue.writeBuffer(indicesBuffer , 0, indices  );\ndevice.queue.writeBuffer(uvBuffer      , 0, uvs      );\n\nvar vertWGSL = document.getElementById('vertex.wgsl').innerHTML;\nvar fragWGSL = document.getElementById('fragment.wgsl').innerHTML;\n\n\n// ----------------------------------------------------------------\n\nlet textureSampler = device.createSampler({\n     minFilter: \"linear\",\n     magFilter: \"linear\"\n});\n\nconst img = document.createElement(\"img\");\nimg.src = 'https://webgpulab.xbdev.net/var/images/footballfield.jpg';\nawait img.decode();\n\nconst basicTexture = device.createTexture({\n    size: [img.width, img.height, 1],\n    format: presentationFormat , // \"bgra8unorm\",\n    usage:  GPUTextureUsage.COPY_DST | GPUTextureUsage.TEXTURE_BINDING\n});\n\nconst imageCanvas = document.createElement('canvas');\nimageCanvas.width =  img.width;\nimageCanvas.height = img.height;\nconst imageCanvasContext = imageCanvas.getContext('2d');\nimageCanvasContext.drawImage(img, 0, 0, imageCanvas.width, imageCanvas.height);\nconst imageData = imageCanvasContext.getImageData(0, 0, imageCanvas.width, imageCanvas.height);\nlet textureData= new Uint8Array( img.width * img.height * 4);\nfor (let x=0; x<img.width * img.height * 4; x++)\n{\n   textureData[ x ] = imageData.data[ x ];\n}\n\ndevice.queue.writeTexture( { texture: basicTexture },\n            textureData,\n            {   offset     :  0,\n                bytesPerRow:  img.width * 4,\n                rowsPerImage: img.height\n             },\n            [ img.width  ,  img.height,  1  ]   );\n\n\n// ----------------------------------------------------------------\n\n// dynamic world transforms (animate/rotate the shape)\nconst rotation   = [0, 0, 0];\nlet rotateXMat   = mat4.create();\nlet rotateYMat   = mat4.create();\nlet rotateZMat   = mat4.create();\n\n\nprojectionMatrix     = mat4.create();\nconst viewMatrix           = mat4.create();\nviewProjectionMatrix = mat4.create();\nmodelMatrix          = mat4.create();\n\nmat4.perspective(projectionMatrix, Math.PI / 2, canvas.width / canvas.height, 0.1, 10.0)\nmat4.lookAt(viewMatrix, [0, 1.4, 1], [0, 0, 0], [0, 1, 0]);\nmat4.multiply(viewProjectionMatrix, projectionMatrix, viewMatrix);\n\nconst vertexUniformBuffer = device.createBuffer({\n  size: 128,\n  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST\n});\n\ndevice.queue.writeBuffer(vertexUniformBuffer,   0,  viewProjectionMatrix );\ndevice.queue.writeBuffer(vertexUniformBuffer,   64, modelMatrix          );\n\n// ----------------------------------------------------------------\n\n\nlet lighting2 = {\n   pos\t\t: [ 0, 1, 0 ],\n   type\t\t: 0,\n   intensity: 1.0\n};\n\n\nconst lightUniformBuffer = device.createBuffer({\n  size:  5 * 4, \n  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST\n});\n\n\nconst lightingdata2  = new Float32Array( Object.values(lighting2).flat() );\n\ndevice.queue.writeBuffer(lightUniformBuffer,   0, lightingdata2  );\n\n\n// ----------------------------------------------------------------\n\nconst sceneUniformBindGroupLayout = device.createBindGroupLayout({\n  entries: [\n    { binding: 0, visibility: GPUShaderStage.VERTEX,   buffer:  { type: \"uniform\"  }   }, \n    { binding: 1, visibility: GPUShaderStage.FRAGMENT, sampler: { type: \"filtering\"}   },\n    { binding: 2, visibility: GPUShaderStage.FRAGMENT, texture: { sampleType: \"float\",\n                                                                  viewDimension: \"2d\"} },\n    { binding: 3, visibility: GPUShaderStage.FRAGMENT, buffer : { type: \"uniform\"    } }\n  ]\n});\n\nconst uniformBindGroup = device.createBindGroup({\n  layout:   sceneUniformBindGroupLayout,\n  entries: [\n    { binding : 0, resource: { buffer: vertexUniformBuffer } },\n    { binding : 1, resource: textureSampler                  },\n    { binding : 2, resource: basicTexture.createView()       },\n    { binding : 3, resource: { buffer: lightUniformBuffer}   }\n   ],\n});\n\n// ----------------------------------------------------------------\n\nconst depthTexture = device.createTexture({\n  size   : presentationSize,\n  format : 'depth24plus',\n  usage  : GPUTextureUsage.RENDER_ATTACHMENT,\n});\n\n// ----------------------------------------------------------------\n\n\nconst pipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({bindGroupLayouts: [sceneUniformBindGroupLayout]}),\n    vertex:   {  module    : device.createShaderModule({ \n                             code : vertWGSL }),\n                 entryPoint: 'main',\n                 buffers    : [ { arrayStride: 12, attributes: [{ shaderLocation: 0,\n                                                                  format: \"float32x3\",\n                                                                  offset: 0  }]        },\n                                { arrayStride: 12, attributes: [{ shaderLocation: 1,\n                                                                  format: \"float32x3\",\n                                                                  offset: 0  }]        },\n                                { arrayStride: 12, attributes: [{ shaderLocation: 2,\n                                                                  format: \"float32x3\",\n                                                                  offset: 0  }]        },\n                                { arrayStride: 8,  attributes: [{ shaderLocation: 3,\n                                                                  format: \"float32x2\",\n                                                                  offset: 0  }]        }\n                              ]},\n    fragment: {  module    : device.createShaderModule({ \n                             code : fragWGSL,     }),\n                 entryPoint: 'main',\n                 targets: [{  format : presentationFormat  }] },\n    primitive: { topology  : 'triangle-list',\n                 frontFace : \"ccw\",\n                 cullMode  : 'none',\n                 stripIndexFormat: undefined },\n    depthStencil: {\n                 depthWriteEnabled: true,\n                 depthCompare     : 'less',\n                 format           : 'depth24plus' }\n});\n\n// GPURenderPassDescriptor \nconst renderPassDescriptor = { \n      colorAttachments:  [{    \n           view     : undefined, // asign later in frame\n           loadOp:\"clear\", clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },\n           storeOp  : 'store' }],\n      depthStencilAttachment: {\n           view: depthTexture.createView(),\n           depthLoadOp:\"clear\", depthClearValue: 1.0,\n           depthStoreOp: 'store',\n           //// // // // // // // // stencilLoadValue: 0,\n           //// // // // // // // // stencilStoreOp: 'store' \n        } \n};\n\n\nsetuptracking();\n\nrotation[1] += 0.1;\n\n// put image in top left to show what the training image looks like\nlet simg = document.createElement('img');\nsimg.id = 'simg';\nsimg.style.position = 'absolute';\nsimg.style.left     = '256px';\nsimg.style.top      = '0px';\nsimg.style.width    = '128px';\nsimg.style.width    = '128px';\ndocument.body.appendChild( simg );\n\nlet ediv = document.createElement('div');\nediv.id = 'ediv';\nediv.style.position = 'absolute';\nediv.style.left     = '128';\nediv.style.top      = '138px';\nediv.style.fontSize = '8pt';\ndocument.body.appendChild( ediv );\n\nlet numtrainingimages = document.getElementById('numtrainingimages').value;\n\nstarttraining = function()\n{\n  resetZip();\n  imgcounter = 0;\n  numtrainingimages = document.getElementById('numtrainingimages').value;\n}\n\n\nvar counter = 0;\nlet lighting = {\n  pos\t\t: [ 0, 1, 1 ],\n  type\t\t: 1,\n  intensity  : 0.2\n};\n\nfunction frame() {\n\n  // --------------------------------------------------\n  // Update uniform buffer \n  \n  let lightingdata  = new Float32Array( Object.values(lighting).flat() );\n  \n  device.queue.writeBuffer(lightUniformBuffer,   0, lightingdata );\n \n  // --------------------------------------------------\n  \n  device.queue.writeBuffer(vertexUniformBuffer,   0,  viewProjectionMatrix );\n\n  \n  //rotation[2] += 0.005;\n  mat4.fromXRotation(rotateXMat, rotation[0]);\n  mat4.fromYRotation(rotateYMat, rotation[1]);\n  mat4.fromZRotation(rotateZMat, rotation[2]);\n\n  mat4.multiply(modelMatrix, rotateYMat,     rotateZMat);\n  mat4.multiply(modelMatrix, modelMatrix,    rotateXMat);\n\n  device.queue.writeBuffer(vertexUniformBuffer, 64, modelMatrix);\n\n  // --------------------------------------------------\n  renderPassDescriptor.colorAttachments[0].view = context.getCurrentTexture().createView();\n\n  const commandEncoder = device.createCommandEncoder();\n\n  const renderPass = commandEncoder.beginRenderPass(renderPassDescriptor);\n  renderPass.setPipeline(pipeline);\n\n  renderPass.setBindGroup(0, uniformBindGroup);\n  renderPass.setVertexBuffer(0, positionBuffer);\n  renderPass.setVertexBuffer(1, colorBuffer);\n  renderPass.setVertexBuffer(2, normalBuffer);\n  renderPass.setVertexBuffer(3, uvBuffer);\n  renderPass.setIndexBuffer(indicesBuffer, 'uint32');\n  renderPass.drawIndexed(6, 1, 0, 0);\n\n  renderPass.end();\n  device.queue.submit([commandEncoder.finish()]);\n\n  // --------------------------------------------------\n  \n  // process and store the generated image data\n  \n  let idealCoords = drawOverlay();\n  \n  let scrnpng = canvas.toDataURL(\"image/jpeg\"); // alt png\n  //console.log( scrnpng );\n  simg.src = scrnpng;\n\n  \n  \n  counter++;\n  if ( counter % 20 == 0 && (imgcounter>-1) && (imgcounter<numtrainingimages) )\n  {\n    lighting.pos.y =  1 + Math.random()*2;\n    lighting.pos.x = -2 + Math.random()*4;\n    lighting.pos.z = -2 + Math.random()*4;\n    lighting.pos.type = (Math.random>0.5) ? 0 : 1;\n    lighting.intensity = 0.2 + Math.random()*1.0;\n\n    imgcounter++;\n    document.getElementById('curimage').innerHTML = 'Current Image:' + imgcounter;\n    \n    let ly = 0.5 + Math.random()*2.5;\n    let lz = 0.5 + Math.random()*0.5;\n    let aspec = (Math.PI / 2) + (-0.1+Math.random()*0.2);\n\n    mat4.perspective(projectionMatrix, aspec, canvas.width / canvas.height, 0.1, 10.0)\n    mat4.lookAt(viewMatrix, [0, ly, lz], [0, 0, 0], [0, 1, 0]);\n    mat4.multiply(viewProjectionMatrix, projectionMatrix, viewMatrix);\n\n    rotation[0] = 0 + Math.random()*0.5;\n    rotation[1] = 1 + Math.random()*2;\n\n    addImageDataList( scrnpng, idealCoords );\n    \n    if ( imgcounter == numtrainingimages )\n    {\n       createZip();\n    }\n  }\n  \n\n  // if you want constant updates (animated) - keep refreshing\n  requestAnimationFrame(frame);\n};\n\nframe();\n\n\n\t \n\t \n\t \n\t \n\t \n\t \n\t \n\t \n\t \n\t \n\t "},{"tabname":"vertex.wgsl","tabcontents":"struct Uniforms {\n  modelViewProjectionMatrix : mat4x4<f32>,\n  modelMatrix               : mat4x4<f32>,\n};\n@binding(0) @group(0) var<uniform> uniforms : Uniforms;\n\nstruct VSOut {\n    @builtin(position) Position: vec4<f32>,\n    @location(0)       tpos    : vec4<f32>,\n    @location(1)       color   : vec3<f32>,\n    @location(2)       normal  : vec3<f32>,\n    @location(3)       uvs     : vec2<f32>,\n};\n\n@vertex \nfn main(@location(0) inPos  : vec3<f32>,\n        @location(1) color  : vec3<f32>,\n        @location(2) normal : vec3<f32>,\n        @location(3) uvs    : vec2<f32>) -> VSOut  \n{ \n  var pos = uniforms.modelViewProjectionMatrix * uniforms.modelMatrix * vec4<f32>( inPos, 1.0);\n  var vsOut: VSOut;\n  vsOut.Position = pos;\n  vsOut.tpos     = pos;\n  vsOut.uvs      = uvs;\n  vsOut.color    = color;\n  vsOut.normal   = (uniforms.modelMatrix * vec4<f32>(normal, 0.0)).xyz;\n  return vsOut;\n\n}\n\t \n\t \n\t \n\t \n\t \n\t \n\t "},{"tabname":"fragment.wgsl","tabcontents":"@group(0) @binding(1) var mySampler: sampler;\n@group(0) @binding(2) var myTexture: texture_2d<f32>;\n\nstruct Uniforms {\n  lpos : vec3<f32>,\n  ltype: f32,\n  lpow : f32\n};\n@group(0) @binding(3) var <uniform> lighting : Uniforms;\n\n//let lightPos : vec3<f32> = vec3<f32>(0.0, 2.0, 0.0);\n\n@fragment\nfn main(@location(0) tPos   : vec4<f32>,\n    \t@location(1) inColor: vec3<f32>,\n        @location(2) normal : vec3<f32>,\n        @location(3) uvs    : vec2<f32>) -> @location(0) vec4<f32> \n{\n    let pos : vec3<f32> = tPos.xyz / tPos.w;\n        \n    // return vec4 (rgba)\n    let texCol = textureSample(myTexture, mySampler, uvs );\n \n    var fragCol = texCol;\n    \n    if ( lighting.ltype == 0 )\n    {\n      let lightdir = normalize(pos - lighting.lpos);\n\n      // scale brightness based on the normal vs ref drection\n      let illum = abs( dot( lightdir, normal ) );\n\n      // use texture for the color (scale it by the illum value)\n      fragCol = vec4<f32>( texCol.xyz * illum , 1.0 ); \n    }\n    else if ( lighting.ltype == 1 )\n    {\n        let lightdist = pos - lighting.lpos;\n        \n        let lightdistsq = dot( lightdist, lightdist );\n        \n        let intensity = 1.0 / (1.0 + lightdistsq*lighting.lpow);\n        \n        let lightdir = normalize(pos - lighting.lpos);\n        \n        let illum = abs( dot( lightdir, normal ) ) * intensity;\n        \n        fragCol = vec4<f32>( texCol.xyz * illum , 1.0 );\n    }\n    \n    \n    return fragCol;\n}\n\n\t \n\t \n\t \n\t \n\t \n\t \n\t "},{"tabname":"calculateworldpoints.js","tabcontents":"console.log('calculateworldpoints.js');\n\n/*\n  Calculate the screen position (projected 3d locations)\n  Also display a '2d' canvas to check that the calcualted positions are right.\n*/\n\ncanvas2d  = document.createElement('canvas');\ncanvas2d.width = canvas2d.height = 256;\ndocument.body.appendChild( canvas2d );\ncanvas2d.style['z-index'] = 99999;\ncanvas2d.style.position = 'absolute';\ncanvas2d.style.left = '0px';\ncanvas2d.style.top  = '0px';\n\ncontext2d = canvas2d.getContext('2d');\n\n// preload the transform libray\nlet pp = await fetch( 'https://cdnjs.cloudflare.com/ajax/libs/gl-matrix/2.6.0/gl-matrix-min.js' );\nlet tt = await pp.text();\n\n// run the tranform library\nvar script = document.createElement('script');\nscript.type  = 'text/javascript';\nscript.innerHTML = tt;\ndocument.head.appendChild(script); \n\ntrackingPointsScreen = [];\ntrackingPointsWorld  = [];\n\nsetuptracking = function()\n{\n  /*\n    Points in pixel coordinates (flat 2d texture)\n    See: https://notebook.xbdev.net/index.php?page=userdefinepoints&\n  */\n  let trackingPointsTex = [\n    /*\n    [ 32, 21 ],\n    [ 32, 431],\n    [304, 430],\n    [578, 430],\n    [582, 21 ],\n    [305, 21 ]*/\n   [33, 20],\n   [306, 20],\n   [580, 21],\n   [579, 134],\n   [580, 184],\n   [579, 266],\n   [579, 319],\n   [581, 431],\n   [32, 432],\n   [32, 318],\n   [31, 266],\n   [32, 184],\n   [32, 133],\n   [108, 134],\n   [108, 194],\n   [108, 256],\n   [109, 319],\n   [503, 134],\n   [503, 193],\n   [503, 259],\n   [503, 315],\n   [306, 182],\n   [263, 222],\n   [349, 224],\n   [306, 268],\n   [306, 226]\n  ];\n  let sz = [612, 453];\n\n  // plane is at 0 height\n  // test dimensions of the rectangle -1 to 1 (we can calculate the coordinates)\n\n  trackingPointsWorld = [];\n  for (let i=0; i<trackingPointsTex.length; i++)\n  {\n      let z = -1 + 2 * trackingPointsTex[i][0]/sz[0];\n      let y = 0.0;\n      let x = -1 + 2 * trackingPointsTex[i][1]/sz[1];\n      trackingPointsWorld.push( [ x, y, z ] );\n  }\n}// end setuptracking(..)\n\n\ndrawScreenPoint = function(xx, yy, col)\n{\n    context2d.beginPath();\n    // arc(x, y, radius, startAngle, endAngle)\n    context2d.arc(xx, yy,  5,  0, 2*Math.PI );\n    context2d.fillStyle = col;\n    context2d.fill();\n    context2d.lineWidth = 1;\n    context2d.strokeStyle = '#003399';\n    context2d.stroke();\n}\n\n\ndrawOverlay = function()\n{\n  outMatrix = mat4.create();\n  mat4.multiply(outMatrix, viewProjectionMatrix, modelMatrix );\n  //console.log('mat4:', outMatrix );\n  \n  trackingPointsScreen = [];\n  for (let i=0; i<trackingPointsWorld.length; i++)\n  {\n      let v = vec4.create();\n      v[0] = trackingPointsWorld[i][0];\n      v[1] = trackingPointsWorld[i][1];\n      v[2] = trackingPointsWorld[i][2];\n      v[3] = 1.0;\n    \n      let res = vec4.create();\n      vec4.transformMat4(res, v, outMatrix );\n      res[0] /= res[3];\n      res[1] /= res[3];\n      res[2] /= res[3];\n      res[3] /= res[3];\n      // console.log( res );\n    \n      let x = res[0]*0.5 + 0.5;// 0-1\n      let y = res[1]*0.5 + 0.5;//0-1\n    \n      x *= canvas2d.width;\n      y  = (y-1.0) * -canvas2d.height;\n    \n      trackingPointsScreen.push( [x, y] );\n  }\n  \n  context2d.clearRect(0, 0, canvas2d.width, canvas2d.height);\n  \n  // console.log('trackingPointsScreen:', trackingPointsScreen );\n  for (let i=0; i<trackingPointsScreen.length; i++)\n  {\n    let sx = trackingPointsScreen[i][0];\n    let sy = trackingPointsScreen[i][1];\n\n    drawScreenPoint( sx, sy, 'orange' );\n    /*\n    context2d.beginPath();\n    // arc(x, y, radius, startAngle, endAngle)\n    context2d.arc(sx, sy,  5,  0, 2*Math.PI );\n    context2d.fillStyle = 'orange';\n    context2d.fill();\n    context2d.lineWidth = 1;\n    context2d.strokeStyle = '#003399';\n    context2d.stroke();\n    */\n  }\n  \n  return trackingPointsScreen; // [x,y]\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t \n\t \n\t \n\t \n\t \n\t \n\t "},{"tabname":"trainingdata.js","tabcontents":"console.log('trainingdata.js');\n\ngenerateNumberImages = 20;\n\nlet zprom = await fetch( 'https://cdnjs.cloudflare.com/ajax/libs/jszip/3.6.0/jszip.min.js' );\nlet zcont = await zprom.text();\n\nvar script = document.createElement('script');\nscript.type = 'text/javascript';\nscript.innerHTML = zcont;\ndocument.head.appendChild(script); \n\nmyzip = new JSZip();\nconsole.log('myzip:', myzip );\n\nlet myzipfolder = myzip.folder(\"trainingimages\");\n\nimgcounter = -1;\ncsvfiledata = '';\n\nresetZip = function()\n{\n  imgcounter = -1;\n  csvfiledata = '';\n  let down = document.getElementById('downloadlink');\n  if ( down ) down.parentElement.removeChild( down );\n}\n\naddImageDataList = async function( imgdata, points )\n{\n  let imgfilename = 'img' + imgcounter + '.jpg';\n  \n  csvfiledata += `${imgfilename}, ${points.flat()}` + '\\n';\n  \n  // You're generating a data uri, which has schema, mime-type etc before the actual base64 data data:[<MIME-type>][;charset=<encoding>][;base64],<data>. You'll have to remove all the content before the data then use it.\n  //myzipfolder.file( imgfilename, imgdata );\n\n  myzipfolder.file(imgfilename, imgdata.substr(imgdata.indexOf(',')+1), {base64: true});\n}\n\n\ncreateZip = async function()\n{\n   console.log('creating zip..');\n  \n   myzip.file(\"imagepoints.csv\", csvfiledata);\n\n   let content = await myzip.generateAsync({type: \"blob\"});\n\n   var a = document.createElement(\"a\");\n   document.body.appendChild( a );\n   a.id = 'downloadlink';\n   a.style.position = 'absolute';\n   a.style.bottom = '20px';\n   a.style.left   = '10px';\n   a.innerHTML = 'Click to Download Training Data Zip';\n \n   var file = new Blob([ content ], {type: 'octet/stream'});\n   a.href = URL.createObjectURL( file );\n   a.download = 'trainingdata.zip';\n   a.style['font-size'] = '20pt';\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t \n\t \n\t \n\t "},{"tabname":"index.html","tabcontents":"<div style='position:absolute;left:10px;top:300px'>\n\nNumber of Training Images:<input id='numtrainingimages' type='number' value='300'><br>\n<div id='curimage'>Current Image: 0</div><br>\n<button onclick='starttraining();'>Start Training</button><br>\n<br>\nAfter the generated images and their associated keypoint feature details have been created you can download them as a single zip.\n</div>\n\t \n\t "},{"tabname":"about.md","tabcontents":"# About\n\nExample code that generates synthetic training data (images and points).  The images and corresponding keypoint markers are created for use later in an offline neural network training program.\n\nFootball pitch from different viewing angles and the 'screen space' location (in pixels) of key points. \n\t \n\t \n\t "}]}